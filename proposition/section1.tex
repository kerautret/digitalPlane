\section{Proposal's context, positioning and objective(s)}
\label{sec:context}

\subsection{Objectives and scientific hypotheses}

\Comments{Present the objectives and the research hypotheses ; present the scientific and technical barriers to be lifted ; present the expected results; if applicable describe any final products developed.}

\noindent\textbf{Context.}
3D volumes come from the segmentation of magnetic resonance, X-ray tomographic or micro-tomographic images. 
They are also generated in scientific modelling and by voxel editors. 
This project is about the geometry of volume boundaries, called \emph{digital surfaces} (fig.~\ref{fig:snow}). 
Keeping the digital nature of the data is an advantage
to use efficient spatial data structures such as voxel octree, 
to perform constructive solid geometry operations,
to do integer-only and exact computations, etc.
A drawback is its poor geometry, because at any resolution a digital surface is only 
made up of quadrangular surface element (\emph{surfel} for short) 
whose normal vector is parallel to one axis. 
Many tasks in computer graphics, vision and 3D image analysis require a richer geometry: 
rendering, surface deformation for simulation or tracking, precise geometric measurements, etc.
To perform relevant geometric tasks and 
to benefit from the above-mentioned advantages in the same time, 
we need to enhance the geometry of digital surfaces by estimating extra data for each surfel. 
This project focuses on estimations of local and first-order geometric quantities: 
normal vector, distance to boundary, coverage of the inner and outer incident voxels 
(see fig.~\ref{fig:2D} for a 2D illustration).  

\begin{figure}[hb]
 \includegraphics[height=0.2\textheight]{snow-compositionv2.png} %width=0.55\textwidth
 \caption{(a) ``ice-air'' interface in a micro-tomographic image of
   snow\protect\footnotemark~with a closer view to a local normal vector
   estimated by computing a relevant facet.} 
\label{fig:snow} 
\end{figure}
\footnotetext{obtained by the 3SR Lab and CEN/CNRM - GAME URA 1357/M\'{e}t\'{e}o-France - CNRS, 
shared during ANR-11-BS02-009 DigitalSnow project.}

\noindent\textbf{Scientific bottleneck.}
The surface geometry within a patch around each surfel should be gathered to provide such estimations
-- \eg by polynomial fitting \cite{Cazals2005,Cazals2008}.
Almost all methods require at least one parameter that controls the size of the patch.  
On the other hand, this project aims at providing \emph{accurate} and \emph{parameter-free} estimators
based on a surface patch with \emph{adaptive} size.
%Note that accuracy will be evaluated in a \emph{multigrid-convervence} framework as described in section \ref{sec:wp}, WP2. 
Since we are looking for first-order estimations, the patch will be typically a piece of digital plane
that locally fits to the digital surface. %(fig.~\ref{sub:pattern}).
A challenge is to cover the whole digital surface by maximal pieces of digital plane. 
What makes the problem hard is that there is a combinatorial explosion
of maximal pieces of digital plane \cite{Sivignon2009} and that among them,
not all are tangent to the digital surface in a point set framework.  
An opportunity to make a breakthrough in this issue is the recent development
by the principal investigator and its collaborators of \emph{plane-probing}
algorithms \cite{LPRTCS2016, LPRDGCI2016, LPRJMIV2017}. These algorithms decide
on-the-fly how to probe the digital surface and make grow a piece of digital plane,
which is tangent by construction. The growth direction is given by both arithmetic and geometric properties.

\noindent\textbf{Objectives and expected results.}
The first goal of this project is to study extra arithmetic and combinatorial properties
of plane-probing algorithms. We expect to design an ultimate plane-probing algorithm that
only probes a part of digital surface \emph{as small as possible} to provide a relevant facet.
A formal definition of what is meant by \emph{small} and a theoretical upper bound should be given.
In addition, we expect to be able to make explicit an underlying minimal and connected piece of
digital plane in order to correctly process convex, concave and saddle parts. 
%(see WP0 and WP1 in section~\ref{sec:wp}, describing work packages).

The second goal is to derive \emph{efficient}, \emph{accurate} and \emph{parameter-free} estimators
of \emph{local} and \emph{first-order} geometric quantities: normal vector (and surfel area as a by-product),
distance to boundary, voxel coverage (fig.~\ref{fig:2D}). We expect to derive such estimators from
the above-mentionned plane-probing algorithm and provide a theoretical evaluation of their accuracy
with respect to resolution. Plane-probing algorithms and derived esimators will be implemented in \DGtal. % at T+24 ?
Theoretical studies and results will be gathered in papers submitted to high impact venues or journals. 
%(WP2).

\begin{figure}[hb]
  \centering
\subfloat[]{ \includegraphics[width=0.17\textwidth,page=1]{square.pdf} } \hspace{0.05\textwidth}
\subfloat[]{ \includegraphics[width=0.17\textwidth,page=2]{square.pdf} } \hspace{0.05\textwidth}
\subfloat[]{ \includegraphics[width=0.17\textwidth,page=3]{square.pdf} } \hspace{0.05\textwidth}
\subfloat[]{ \includegraphics[width=0.17\textwidth,page=4]{square.pdf} } 
 \caption{2D illustration of a digital surface: voxels are big squares whose center is depicted by a black (resp. white) disk if it lies inside (resp. outside) the volume; surfels are elongated dark rectangles. We want to estimate a relevant normal vector at a given surfel (b), but also locally reconstruct a boundary perpendicular to this normal vector in order to derive distance and coverage estimations (c-d).} 
\label{fig:2D} 
\end{figure}

The third goal is to provide a method and a tool for an automatic and \emph{multiscale} analysis of digital surfaces,
based on the number or the size of the computed facets for several subsampled versions of the input 3D volume. 
We expect to develop a tool that provides, without any parameter, the scale at which noise is unlikely.
This tool will be implemented in \DGtalTools~ and published in \IPOL. 
%(WP3). 

%\todo{expected results and final products}

%1-WP0: algorithm purely local, from any starting point, which leads to a reduced basis and retrieves any normal
%1-WP1: pattern generation: minimal number of tiles, connected, 
%2-WP2: estimateur + ordre de convergence theorique + applis ?
%3-WP3: outils qui prend un volume et sors un sous-échantillon non bruité, ou un ensemble de voxels à différentes tailles (publi IPOL)
%=> for each publication and implementation in DGtal. 

%transition not relevant here
%Since there are so many perspectives and paths to follow, this project needs to strengthen a team of experts by full-time workers in order to make the best of them. 

\subsection{Originality and relevance in relation to the state of the art}

\Comments{Emphasise the ambitious nature of the proposal and the novelty of the research in relation to the state of the art ; show the possible contributions of project partners to the state of the art ; present any preliminary results. In the case of a project proposal following up on previous project(s) already funded by ANR or by another body, provide a summary of the results achieved and clearly describe the new issues raised and the new objectives set out in the light of the earlier project.}

In this section, we underline the novelty of the proposed approach in relation to the state of the art.  
We focus on the estimation of the normal direction, which is the most challenging task with the biggest impact. 
Other first-order quantities are either obtained as a by-product of the normal estimation (\eg surfel area)
or carry position information already approximated by the digital surface itself (\eg distance to boundary).
%In this last case, it seems unlikely that the accuracy could be improved by an order of magnitude. 

In addition, a digital surface is a quadrangular mesh. The vertex set consists of evenly spaced data points
whose coordinates are (half-)integer. This specific point cloud approximates a continuous 2-manifold under a
uniform noise model due to discretisation erros. That is why we review the most common methods for normal estimation
not only on digital surfaces, but also on meshes and point clouds.
%TODO nombreuses applications de normal estimation ce qui explique qu'il y ait beaucoup de travaux.  
Note that we do not present 2D estimators (\eg \cite{Provot2011,Esbelin2011,Esbelin2016}),
with the exception of \cite{Lachaud2007}, which is parameter-free and used in 3d estimators
\cite{Lenoir1996,Tellier1999,Lachaud2003}. 

Since one goal of the project is to propose an accurate and parameter-free normal estimator,
we focus in this state of the art on input parameters and, for digital surfaces only, we check
whether the estimator in multigrid-convergent or not. Indeed, the accuracy of a multigrid-convergent estimator
depends on the grid step: the smaller the grid step, the more accurate the estimator. 


%% \noindent\textbf{Digital surface and multigrid convergence.}

%% 3D volumes are collections of cubes. % of size $h$, called \emph{grid step}. 
%% Their topological boundary is a quadrangular mesh called \emph{digital surface}.
%% The vertex set consists of evenly spaced data points whose coordinates are
%% half-integer. This set approximates a continuous 2-manifold under a uniform
%% noise model. 
%% %TODO: Lachaud Thibert

%% Most of the time, when we are working with a digital surface, we are 
%% interested in the geometry of a continuous shape whose digitization
%% is the input 3D volume.
%% Formally, given a compact shape $X \subset \R^3$,
%% its digitization at grid step $h \in \R^+$ is $\Dig(X) := \{z \in \Z^3, hz \in X\}$.
%% If we denote the axis-aligned closed cube centered on $z \in h\Z^3$ and of size $h$ as $Q_z^h$,
%% the cube embedding of a digital set $Z$ at grid step $h$ is $\underset{z\in Z}{\cup}Q_z^h$.
%% Let $\partial_h X$ be the topological boundary of the embedding of the digitization of $X$: 
%% \[
%% \partial_h M := \partial \Big( \underset{z\in\Dig(X)}{\bigcup}Q_z^h \Big).
%% \]

%% We expect that a given geometric quantity, such as a normal vector,
%% computed at a point of a digital surface ($\partial_h X$),
%% is close to the one of the underlying continuous shape ($X$) at a close enough point. 

%% \begin{Definition}[multigrid-convergent estimator \cite{Coeurjolly2012}]
%%   \label{def:multigrid-convergence2}
%%   The estimator $\hat{Q}$ is {\em multigrid-convergent} for the family
%%   {\Shapes} if and only if, for any shape $X \in \Shapes$,
%%   there exists a grid step $h_X>0$ such that the estimate
%%     $\hat{Q}(\Dig(X),y,h)$ is defined for all
%%   $y \in \partial_h X$ with $0<h < h_X$, and for any $x \in \BT{X}$,
%%   \begin{equation*}
%%     \forall y \in \partial_h X \text{~with~} \| y - x \|_1 \le h, \quad
%%     \hat{Q}(\Dig(X),y,h) - Q(X,x) | \le \tau_{X,x}(h),
%%   \end{equation*}
%%   where $\tau_{X,x}: \R^{+*} \rightarrow \R^+$ has null limit at
%%   $0$.
%%   %% This function defines the speed of convergence of $\hat{Q}$
%%   %% toward $Q$ at point $x$ of $\BT{X}$. The convergence is {\em uniform} for
%%   %% $X$ when every $\tau_{X,x}$ is bounded from above by a function
%%   %% $\tau_X$ independent of $x \in \BT{X}$ with null limit at $0$.
%% \end{Definition}

%% The accuracy of a multigrid-convergent estimator depends on the grid step:
%% the smaller the grid step, the more accurate the estimator. 

\subsubsection{Normal estimation on point clouds, meshes and digital surfaces}

\noindent\textbf{Fitting.}
Hoppe \etal estimate the normal at a given data point by computing
the least squares best fitting plane to a point set within a neighborhood
around the point of interest \cite{Hoppe1992}.
Other fitting surfaces have been used too, such as jets, \ie truncated Taylor expansion
of a surface expression \cite{Cazals2005,Cazals2008}. 

All fitting methods first consist in collecting the points used for the fitting.
In the mesh case, a breadth-first search visit the neighbors until enough points
have been collected. In the point-cloud case, we typically resort to the k-nearest-neighbors
strategy. In both cases, the number of points to collect is usually a user-defined parameter,
even if some heuristics are proposed to select it automatically \cite{Hoppe1992,Cazals2005}.  

%In 3d, a straightforward application
%of \cite{Cazals2005}[Theorem 3] to digital surface shows that a polynomial fitting of degree $n$
%estimates the coefficients of the unit normal vector in $O(h^n)$.
%TRIS probablement faux

\noindent\textbf{Voronoi diagram.}
Instead of approximating the tangent space, another familly of methods focuses on the
Voronoi diagram to estimate at best the orthogonal space. Amenta \etal first use the
furthest vertex of the Voronoi cell to estimate the normals of point clouds \cite{Amenta1999}.
In order to get more stable estimates, Alliez \etal propose to apply linear fitting
to the Voronoi cell or to the union of Voronoi cells into a neighborhood \cite{Alliez2007}. 
Mérigot \etal propose an improvement, called Voronoi Covariance Measure (VCM),
by taking a weighted average of covariance matrices of Voronoi cells instead of
taking the covariance matrix of their union \cite{Merigot2011}. In their method,
only the intersection between the Voronoi diagram and a ball around the data point
are taken into account in order to get purely local information about the surface geometry.
A digital variant has been proposed by Cuel \etal \cite{Cuel2015}. 

\noindent\textbf{Integral invariants.}
In the mesh case, another method sums up the surface geometry within a ball by 
computing integrals over the intersection between the ball and the volume
bounded by the mesh \cite{Pottmann2009}. The covariance matrix of the intersection set 
provides a way to estimate principal curvatures, principal directions and normal direction.
A digital variant has been proposed by Lachaud \etal \cite{Lachaud2017}.

The radius of the ball is a user-defined parameter in these methods. The above-mentionned
digital variants \cite{Cuel2015,Lachaud2017} are multigrid convergent for digitization
of smooth shapes if the radius is conveniently chosen with respect to the grid step.

\noindent\textbf{Convolution.}
TODO ?
%pas d'isotropie, donc filtres
%bilateral mesh denoising
%\cite{Fourey2009}

\noindent\textbf{Randomized Hough Transform.}
To end, let us mention also probabilistic methods based on the Randomized Hough Transform.
Boulch and Marlet consider many random triples of data points in a neighborhood
and bin their normal direction into a spherical histogram \cite{Boulch2012}.
Maximal vote provides the normal estimate. However, their method has many input parameters,
the first of which is the size or radius of the neighborhood. 

\subsubsection{Parameter-free estimators on digital surfaces and digital plane segments}

All previous methods require at least one parameter that controls the size of the neighborhood.
On the other hand, the specificity of digital surfaces makes the development of parameter-free
estimators possible. The trick is to perform a uniform fitting of the data points by a plane.
In addition, instead of specifying \emph{a priori} the number of data points, they are added
step by step, while a maximal admissible error is not reached. This threshold is not a user-defined
parameter, but a known constant, because the data points follow a known uniform noise model
in case of digital surfaces. 
%In addition, due to the specific structure of the data points, we can expect to take profit
%TODO utiliser prop arith et combi pour algo efficaces ?

\noindent\textbf{Parameter-free estimators.}
In 2D, the set of maximal digital straight segments (DSSs) \cite{Feschet1999,Feschet2005} provides
a parameter-free and multigrid-convergent tangent (and normal) estimator \cite{Lachaud2007}.
This estimator preserves the inflexion points because maximal DSSs are closely related to local convexity \cite{Roussillon2011}
and normal integration over the digital curve yields a multigrid-convergent length estimator \cite{Coeurjolly2004}. 
In addition, asymptotic properties of maximal DSSs can be used to estimate the local amount of noise along the
digital curve \cite{Kerautret2012}.  

In higher dimension, to the best of our knowledge, only one parameter-free normal estimator
has been proposed in 3D \cite{Lenoir1996,Tellier1999} and extended to nD \cite{Lachaud2003}.
It is based on maximal DSSs on 2D slices.
Maximal DSSs provide windows of adaptive size
but the slicing truncates the geometric information and leads to
an artificial spatial variability because two neighbor surfels only
share one slice over two. 

Given an estimator with an input size parameter, a general strategy to automatically and adaptively select
the parameter value consists in taking the size of maximal DSSs. 
The idea of such hybrid method has been used first in \cite{Devieilleville2009}
for a comparative study of 2D normal estimators and then in \cite{Coeurjolly2014IIfree}
for mean curvature estimation by integral invariants.
In 2D, this strategy is quite interesting for higher-order estimators such as curvature,
but is totally disproportionate for normal estimation since maximal DSSs straightforwardly give a good estimator. 
In 3D, even if this hybrid method may be useful for normal estimation, its computation cost may be rather high,
because we must take into account the preprocessing cost to approximate the parameter value for each surfel,
and the cost of the original estimator without any optimization trick that takes profit of the neighboring
computations with the same parameter value, as done in \cite{Lachaud2017}. 

Finally, a set of digital plane segments (DPSs) that locally fits to the digital surface
would overcome these limitations. The challenge is to efficiently recognize DPSs and to
cover the whole digital surface by a relevant set of DPSs.

\noindent\textbf{Recognition of digital plane segment.}
A \emph{digital plane} is an infinite digital set that 
consists of several consecutive and parallel layers of coplanar points. 
It is defined by a (nonzero) normal $\vec{n} \in \Z^3$ and a position $\mu \in \Z$ as follows:  
$\Plane{\mu}{\vec{n}} := \{ \vec{x} \in \Z^3 \ | \ \mu \leq \vec{x} \cdot \vec{n} < \mu + \|\vec{n}\|_1 \}$
\cite{reveilles1991}.

Given a finite digital set $\Set$, the \emph{recognition problem} consists in providing
$\mu$ and $\vec{n}$ such that $\Set \subseteq \Plane{\mu}{\vec{n}}$ if any.
Note that such a recognition problem can have zero or infinitely many solutions.
One solution can be found in linear time, \ie in $O(|\Set|)$, by linear programming
and all solutions can be found in $O(|\Set|\log{(|\Set|)})$ by computational geometry tools.
See \cite{Brimkov2007} for a review on digital planarity.

However, we usually want to add data points step by step and incrementally update the solution
or the set of solutions. In this on-line framework, algorithms with optimal bounds are difficult
to implement and are in all probability slow in practive due to a high constant (see for instance
\cite{Buzer2003}).
On the other hand, there exists fast geometrical algorithms but with higher theoretical bounds,
\eg \cite{Gerard2005, Charrier2008, Veelaert2012}.
%% the chord algorithm by Gérard \etal \cite{Gerard2005}, 
%% the COBA algorithm by Charrier and Buzer \cite{Charrier2008}
%% the combinatorial algorithm of Veelaert \cite{Veelaert2012}.

%\cite{Gerard2009} preimage
%\cite{Stojmenovic1991} separation
%Provot2006, largeur

Actually, the main challenge is not so much to recognize DPSs, but more to find which data points
should be taken into account during the recognition process to obtain DPSs tangent to the digital
surface.
Segmentation methods usually make grow a point set from a seed by a breadth-first search according
to some heuristics and decide whether the current set is a piece of digital plane or not
\cite{Klette2001,Sivignon2004,Provot2009,Charrier2011}.
The results are however highly dependent on the chosen heuristics.   
%\cite{Sivignon2009} combinatorial explosion of DPSs

Until recently, artihmetic properties of digital planes did not help so much.
Pionneering incremental recognition algorithms based on arithmetic properties \cite{Debled1994,Mesmoudi2002}
are neither as easy-to-implement nor as fast as purely geometric ones, such as \cite{Gerard2005}.  
The work of T. Fernique et V. Berth\'{e} \cite{Fernique2009}, 
based on multidimensional continued fractions and desubstitution on words,
is quite interesting from a theoretical point of view. However, their algorithm,
which reduces a piece of digital surface until no transformation is possible,
is not of practical interest because it requires the whole point set to be known
in advance and must be coupled with another recognition algorithm to conclude
at the last step.  

\noindent\textbf{Preliminary results: plane-probing algorithms.}
An opportunity to make a breakthrough in this issue is the recent development
by the principal investigator and its collaborators of \emph{plane-probing
algorithms} \cite{LPRTCS2016, LPRDGCI2016, LPRJMIV2017}. These algorithms decide
on-the-fly how to probe the digital surface and make grow a piece of digital plane,
which is tangent by construction. The growth direction is given by both arithmetic
and geometric properties.

More formally, given a digital plane $\Plane{\mu}{\vec{n}} \subset Z^3$
and a starting point $\vec{p} \in \Set$, 
a plane-probing algorithm computes the parameters of a digital plane $\Plane{\mu'}{\vec{n}'}$
containing $\vec{p}$ by sparsely probing $\Plane{\mu}{\vec{n}}$ with the predicate
``is $\vec{x}$ in $\Plane{\mu}{\vec{n}}$?''. The parameters of $\Plane{\mu}{\vec{n}}$
and $\Plane{\mu'}{\vec{n}'}$ are expected to be equal when the algorithm terminates.
%for any $\vec{p} \in \Plane{\mu}{\vec{n}}$. 

The first algorithm of this kind has been proposed in \cite{LPRTCS2016}.
Its principle is to deform an initial unit tetrahedron based at the starting point
with only unimodular transformations. Each transformation is decided by looking
mostly at a few points around the tetrahedron. These points are chosen so that
the transformed tetrahedron lies in $\Plane{\mu}{\vec{n}}$, with the same volume,
but closer to the upper plane
$\UpperPlane{\mu}{\vec{n}} := \{ x \in R^3 \ | \ x \cdot \vec{n} = \mu + \|\vec{n}\|_1 \}$.
At the end of this iterative process, one face of the tetrahedron has an extremal
position in the plane and is thus parallel to $\UpperPlane{\mu}{\vec{n}}$.

New plane-probing algorithms have been proposed in \cite{LPRDGCI2016, LPRJMIV2017}. 
They also iteratively deform an initial tetrahedron and stop when one face is
parallel to $\UpperPlane{\mu}{\vec{n}}$. 
However they differ from our first work on several aspects.  %as illustrated on \RefFig{fig:illustration:algo}. 
First, they are simpler, because they repeat one simple operation instead
of several possible transformations depending on the current onfiguration
as in \cite{LPRTCS2016}.  
Second, one vertex of the evolving tetrahedron is a fixed point lying above the 
starting point and the opposite triangular facet. The position of the evolving 
tetrahedron is thus better controlled than in \cite{LPRTCS2016}. 
Last, a geometric criterion is used so that the evolving tetrahedron is not
too much elongated during the computation.  

%surface
In addition, \cite{LPRJMIV2017} show how to use these algorithms
for digital surface analysis. In this preliminary work, one drawback is
the processing of non-convex parts that requires to associate a piece
of digital plane to the tetrahedron by discretizing its triangular facet
not incident to the fixed point. Generating a relevant piece of digital plane
in the course of the computation may be a neater and more efficient
solution.  

%% Although it is a preliminary work with
%% several drawbacks that we expect to overcome in this project, some fall-back
%% solutions are detailed. 

%% For instance, a procedure for detecting planarity defects
%% is detailed.  and in case of non-convex configuration and propose several strategy 
%% We show how to detect convex/concave/inflexion zones onto a digital surfacewe show  , we also For some well-identified starting points, this algo- rithm stops and outputs only an approximation of the normal to P. We show how to detect such bad starting points and how to connect them to their corresponding facet

%reduction
%% For one variant, called $R$-algorithm, we experimentally observe
%% that the returned basis is reduced at least after every two steps and it is always reduced
%% when the algorithm terminates. This conjecture is not proved yet and it is one of our
%% future work to prove it.

%generation d'un motif
%substitution methode generale
\noindent\textbf{Discretization and pattern generation.} TODO


\subsection{Methodology and risk management}

\Comments{Describe the methods and technical decisions, risks and fall-back solutions envisaged.}

%methodes: 
%plane-probing algorithms sont une bonne approche pour etre tangent, local, sans parametre,
%substitution bonne approche pour generer un motif au cours de l'algorithme
%lien avec env. conv. bonne approche pour preuve estimateur et analyse multi-echelle

%risques:
%algos pas parfaits: il en existe actuellement un 
%\Comments{Le risque est réduit par les travaux exploratoires des participants au projet.}
%motif genere pas bons:
 %- on les genere apres coup en faisant une sorte de croissance de regions
 %- on traite les concavites avec un critere de separation
%l'estimateur n'est pas convergent: peu probable, mais si ça arrive c'est que les motifs sont trop petits, il ne grandisse pas assez vite par rapport au pas h, dans ce cas on agrandit le motif jusqu'à obtenir m surfels, ou on agrandit jusqu'à avoir une épaisseur <= h, ou on combine des motifs voisins.


